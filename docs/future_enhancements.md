# Future Enhancements for MetricMancer

## ğŸ¯ Delta-Based Review Time Estimation

### Problem
Currently, the review strategy estimates time based on the **entire file's complexity**, even when only a small portion of the file has changed. This can lead to:
- Over-estimation of review time for large files with small changes
- Under-prioritization of small files with complex changes
- Less accurate resource planning

### Proposed Solution: Delta Complexity Analysis

Instead of measuring entire file complexity, analyze only the **changed portions**:

#### Implementation Approach

```python
def get_changed_function_complexity(file_path: str, base_branch: str = "main") -> Dict[str, Any]:
    """
    Analyze complexity of only the changed functions in a file.
    
    Returns:
        {
            'delta_complexity': int,        # Complexity of changed functions only
            'changed_functions': List[str],  # Names of changed functions
            'changed_lines': int,            # Number of lines changed
            'complexity_per_function': Dict[str, int]  # Function -> Complexity
        }
    """
    # Step 1: Get changed line ranges from git diff
    diff_output = subprocess.run(
        ['git', 'diff', '--unified=0', f'{base_branch}...HEAD', '--', file_path],
        capture_output=True, text=True
    ).stdout
    
    # Parse diff to get line ranges: [(start, end), ...]
    changed_ranges = parse_diff_ranges(diff_output)
    
    # Step 2: Parse current file with AST to find functions
    with open(file_path, 'r') as f:
        tree = ast.parse(f.read(), filename=file_path)
    
    # Step 3: Map each function to its line range
    functions = {}
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            functions[node.name] = {
                'start_line': node.lineno,
                'end_line': node.end_lineno,
                'complexity': calculate_complexity(node)
            }
    
    # Step 4: Identify which functions overlap with changed ranges
    affected_functions = []
    delta_complexity = 0
    
    for func_name, func_info in functions.items():
        for change_start, change_end in changed_ranges:
            if ranges_overlap(
                (func_info['start_line'], func_info['end_line']),
                (change_start, change_end)
            ):
                affected_functions.append(func_name)
                delta_complexity += func_info['complexity']
                break
    
    return {
        'delta_complexity': delta_complexity,
        'changed_functions': affected_functions,
        'changed_lines': sum(end - start + 1 for start, end in changed_ranges),
        'complexity_per_function': {
            name: functions[name]['complexity'] 
            for name in affected_functions
        }
    }


def parse_diff_ranges(diff_output: str) -> List[Tuple[int, int]]:
    """
    Parse git diff output to extract changed line ranges.
    
    Example diff header:
        @@ -42,7 +42,9 @@ def some_function():
        
    Returns: [(42, 50), ...] # (start_line, end_line)
    """
    import re
    ranges = []
    
    for line in diff_output.split('\n'):
        if line.startswith('@@'):
            # Extract the "+42,9" part (lines in new version)
            match = re.search(r'\+(\d+)(?:,(\d+))?', line)
            if match:
                start = int(match.group(1))
                count = int(match.group(2)) if match.group(2) else 1
                end = start + count - 1
                ranges.append((start, end))
    
    return ranges


def ranges_overlap(range1: Tuple[int, int], range2: Tuple[int, int]) -> bool:
    """Check if two line ranges overlap."""
    start1, end1 = range1
    start2, end2 = range2
    return not (end1 < start2 or end2 < start1)
```

#### Updated Review Time Estimation

```python
def _estimate_review_time_delta(
    self, 
    delta_complexity: int,      # Complexity of changed functions only
    changed_lines: int,          # Number of changed lines
    churn: float,
    risk_level: str
) -> int:
    """
    Estimate review time based on delta complexity (more accurate).
    """
    base_time = 5  # Lower base time since we're only reviewing changes
    
    # Add time based on delta complexity
    if delta_complexity > 30:
        base_time += 45
    elif delta_complexity > 15:
        base_time += 25
    elif delta_complexity > 5:
        base_time += 15
    
    # Add time based on changed lines
    if changed_lines > 200:
        base_time += 30
    elif changed_lines > 100:
        base_time += 20
    elif changed_lines > 50:
        base_time += 10
    
    # Add time based on churn (same as before)
    if churn > 15:
        base_time += 20
    elif churn > 10:
        base_time += 10
    
    # Add time based on risk level (same as before)
    if risk_level == "critical":
        base_time += 30
    elif risk_level == "high":
        base_time += 15
    
    return base_time
```

### Benefits

1. **More Accurate Estimates**: Review time reflects actual changes, not file size
2. **Better Prioritization**: Complex changes in simple files get proper attention
3. **Function-Level Insights**: Know exactly which functions changed
4. **Targeted Reviews**: Reviewers can focus on specific functions
5. **PR Context**: Show "You changed 3 functions with total complexity 42"

### Example Output

```
File: app/analyzer.py
   Risk Level: CRITICAL
   
   ğŸ” Changed Functions:
      - analyze_repository() [Complexity: 15, Lines: 45-89]
      - _process_files() [Complexity: 8, Lines: 120-145]
   
   Delta Complexity: 23 (vs. Full File: 90)
   Changed Lines: 67 (vs. Total: 450)
   Estimated Time: 65 minutes (vs. Full File: 125 minutes)
   
   Focus Areas: analyze_repository complexity, _process_files error handling
```

### Implementation Priority

This feature would be particularly valuable for:
- Large, mature codebases
- Files with high overall complexity
- Teams practicing incremental refactoring
- Pull request reviews (vs. full codebase reviews)

### Language Support Matrix

| **Language/File Type** | **Complexity Analysis** | **Delta Analysis** | **Review Strategy** |
|------------------------|-------------------------|-------------------|---------------------|
| Python | âœ… Full | âœ… Function-level | âœ… Complete |
| JavaScript/TypeScript | âœ… Full | âœ… Function-level | âœ… Complete |
| Java | âœ… Full | âœ… Method-level | âœ… Complete |
| C#/C/C++ | âœ… Full | âœ… Function-level | âœ… Complete |
| Go | âœ… Full | âœ… Function-level | âœ… Complete |
| Ada | âœ… Full | âœ… Procedure-level | âœ… Complete |
| Markdown/Docs | âŒ N/A | âš ï¸ Line-based | âœ… Churn-based |
| JSON/YAML/Config | âŒ N/A | âš ï¸ Structure-based | âœ… Impact-based |
| Other text files | âŒ N/A | âš ï¸ Line-based | âœ… Churn-based |

**Legend:**
- âœ… Full support
- âš ï¸ Partial support (alternative metrics used)
- âŒ Not applicable (but review strategy still works via churn/ownership)

### Related Features to Add

1. **Function-level ownership**: Track which developer owns which functions
2. **Historical delta analysis**: Track how function complexity changes over time
3. **Review coverage**: Ensure all changed functions are reviewed
4. **Complexity trends**: Show if changes increase or decrease complexity
5. **Config file impact analysis**: Assess impact of configuration changes (e.g., dependency updates)

### Technical Considerations

- **Performance**: AST parsing adds overhead (~50-100ms per file)
- **Language support**: Would work for all supported languages with parsers (Python, JS, TS, Java, C#, C, C++, Go, Ada)
- **Non-code files**: For files without complexity parsers (Markdown, JSON, YAML), delta analysis would focus on:
  - Changed line count only
  - Structural changes (e.g., added sections in docs)
  - Configuration impact (e.g., dependency changes in package.json)
- **Edge cases**: 
  - New functions (no baseline to compare)
  - Deleted functions (should count as reviewed)
  - Moved functions (detect refactoring)
  - Configuration files (assess impact, not complexity)
- **Git operations**: Requires git repository and comparison branch

### Configuration

Add to CLI:
```bash
# Enable delta-based complexity analysis
python -m src.main src --review-strategy --review-branch-only --delta-complexity

# Compare with full-file analysis
python -m src.main src --review-strategy --review-branch-only --show-both
```

### Proof of Concept

See `tests/utilities/test_delta_complexity.py` for examples and edge cases.

---

**Status**: ğŸ“‹ Planned for v2.0  
**Effort**: Medium (2-3 days implementation + testing)  
**Value**: High (significantly improves review time accuracy)

---

## ğŸ’» Enhanced Terminal Output Formats

### Current State

The default terminal output shows a file tree with KPIs for each file:
```
. src [Avg. C:16.9, Min C:0, Max C:172, Avg. Churn:5.6]
â”‚   â”œâ”€â”€ analyzer.py [C:90, Churn:20, Hotspot:1800] Owners: ...
â”‚   â””â”€â”€ scanner.py [C:20, Churn:10, Hotspot:200] Owners: ...
```

While this provides detailed information, it can be overwhelming and doesn't immediately highlight actionable insights.

### Proposed Enhancements

#### Option 1: Executive Summary Dashboard (Recommended Default)

Replace the file tree with an actionable summary that gives developers immediate insights:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           METRICMANCER ANALYSIS SUMMARY                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š OVERVIEW
   Files Analyzed:        71
   Total Complexity:      1,200
   Average Complexity:    16.9
   
ğŸ”¥ CRITICAL ISSUES (Immediate Attention Required)
   Critical Hotspots:     6 files
   Top 3 Risk Files:
   1. analyzer.py              (Hotspot: 1800, C:90, Churn:20)
   2. report_data_collector.py (Hotspot: 312, C:26, Churn:12)
   3. code_churn.py            (Hotspot: 297, C:27, Churn:11)
   
âš ï¸  HIGH PRIORITY
   Emerging Hotspots:     3 files
   High Complexity (>15):  15 files
   High Churn (>10):       8 files

ğŸ“ˆ HEALTH METRICS
   Code Quality:          B (70/100)
   Test Coverage:         Unknown (run with --coverage)
   Tech Debt Score:       Medium-High
   
ğŸ’¡ RECOMMENDATIONS
   1. Refactor analyzer.py (critical complexity)
   2. Investigate high churn in report_data_collector.py
   3. Add tests for 6 critical files
   4. Review code ownership for fragmented files
   
ğŸ“ DETAILED REPORTS
   HTML Report:    output/complexity_report.html
   Hotspot Report: Run with --list-hotspots
   Review Strategy: Run with --review-strategy
   
â±ï¸  Analysis Time: 1.04s
```

**Benefits:**
- âœ… Actionable from first glance
- âœ… Shows critical problems first
- âœ… Guides developer toward next steps
- âœ… Provides context for metrics
- âœ… Prioritizes information by importance

**Implementation:**
- Create new `CLISummaryFormatter` class in `src/report/cli/`
- Make it default for `--output-format human`
- Keep current tree view as `--output-format human-tree` or with `--verbose` flag
- Add `--summary` / `--detailed` flags for quick switching

#### Option 2: Top Risks Table

Focus on files that need immediate attention:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        TOP 10 RISK FILES                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”‚ Risk â”‚ File                              â”‚ Hotspot â”‚ Complexity â”‚ Churn â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”´   â”‚ app/analyzer.py                   â”‚  1800   â”‚     90     â”‚  20.0 â”‚
â”‚ ğŸ”´   â”‚ report/report_data_collector.py   â”‚   312   â”‚     26     â”‚  12.0 â”‚
â”‚ ğŸ”´   â”‚ kpis/codechurn/code_churn.py      â”‚   297   â”‚     27     â”‚  11.0 â”‚
â”‚ ğŸŸ¡   â”‚ main.py                           â”‚   242   â”‚     11     â”‚  22.0 â”‚
â”‚ ğŸŸ¡   â”‚ utilities/git_cache.py            â”‚   268   â”‚    134     â”‚   2.0 â”‚
â”‚ ğŸŸ¡   â”‚ report/report_renderer.py         â”‚   276   â”‚     23     â”‚  12.0 â”‚
â”‚ ğŸŸ¢   â”‚ report/cli/cli_report_format.py   â”‚   784   â”‚    112     â”‚   7.0 â”‚
â”‚ ğŸŸ¢   â”‚ app/metric_mancer_app.py          â”‚   700   â”‚     70     â”‚  10.0 â”‚
â”‚ ğŸŸ¢   â”‚ report/json/json_report_format.py â”‚   252   â”‚     36     â”‚   7.0 â”‚
â”‚ ğŸŸ¢   â”‚ utilities/tree_printer.py         â”‚   270   â”‚     27     â”‚  10.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

Total files analyzed: 71  |  Critical: 6  |  High: 9  |  Medium: 12

ğŸ’¡ Use --list-hotspots for full report with detailed recommendations
```

**Benefits:**
- Clear visual hierarchy with emojis
- Table format is easy to scan
- Shows top 10 by default (configurable with `--top N`)
- Immediate focus on high-risk areas

**Implementation:**
- Add as `--output-format risks` or `--top-risks N`
- Could be default for CI/CD pipelines
- Sortable by different columns with flags

#### Option 3: Trend Analysis (Requires History)

Show how metrics change over time:

```
ğŸ“Š CODE HEALTH TRENDS (vs. last week)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric          â”‚ Current â”‚ Previous â”‚ Trend          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Complexity      â”‚ 16.9    â”‚ 14.6     â”‚ â†‘ +2.3  âš ï¸     â”‚
â”‚ Hotspot Files   â”‚ 22      â”‚ 18       â”‚ â†‘ +4    âš ï¸     â”‚
â”‚ Critical Files  â”‚ 6       â”‚ 6        â”‚ â†’ same  âš ï¸     â”‚
â”‚ Average Churn   â”‚ 5.6     â”‚ 6.8      â”‚ â†“ -1.2  âœ…     â”‚
â”‚ Code Quality    â”‚ B (70)  â”‚ C (65)   â”‚ â†‘ +5    âœ…     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸  Warning: Complexity trending upward - consider refactoring sprint
âœ… Improvement: Churn is decreasing - code stabilizing

ğŸ“ˆ Historical Data: 4 weeks available
   Run with --history 12 to see 12-week trends
```

**Benefits:**
- Shows progress over time
- Helps teams see if interventions are working
- Provides motivation when trends improve
- Early warning system for deteriorating code

**Implementation:**
- Store historical metrics in `.metricmancer/history/` folder
- Add `--compare <date>` or `--compare <commit>` flags
- Generate JSON snapshots automatically
- Optional: integrate with git tags/releases

#### Option 4: Quick Win Suggestions

Prioritize improvements by impact and effort:

```
ğŸ¯ QUICK WINS (Highest Impact, Lowest Effort)

1. Extract functions from analyzer.py
   Impact:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ High (8/10)
   Effort:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ Medium (4/10)
   Time:    2-4 hours
   Reason:  Critical hotspot, high complexity (C:90)
   Action:  Break down into smaller functions (<10 complexity each)

2. Add unit tests for report_data_collector.py
   Impact:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ High (7/10)
   Effort:  â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ Low (3/10)
   Time:    1-2 hours
   Reason:  Critical file with high churn, likely lacks test coverage
   Action:  Start with happy path tests, then edge cases

3. Document git_cache.py
   Impact:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ Medium (5/10)
   Effort:  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Low (2/10)
   Time:    30 minutes
   Reason:  Single owner (100%), stable but complex (C:134)
   Action:  Add docstrings and architecture comments

4. Refactor code_churn.py
   Impact:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ High (8/10)
   Effort:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ Medium (5/10)
   Time:    3-5 hours
   Reason:  Critical hotspot, active development area
   Action:  Extract calculation logic into separate functions

ğŸ’¡ Tip: Focus on Quick Wins #2 and #3 for maximum ROI
```

**Benefits:**
- Prioritizes refactoring work
- Shows expected time investment
- Balances impact vs. effort
- Helps with sprint planning

**Implementation:**
- Calculate impact based on hotspot score, churn, ownership
- Estimate effort based on complexity and file size
- Add `--quick-wins N` flag to show top N opportunities
- Consider team velocity and preferences

#### Option 5: Hybrid Summary + Expandable Details

Combine concise summary with ability to drill down:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ANALYSIS SUMMARY                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Files: 71 | Critical: 6 ğŸ”´ | High: 9 ğŸŸ¡ | Medium: 12 ğŸŸ¢ | Low: 44 âšª

ğŸ”¥ TOP 3 RISKS
   1. app/analyzer.py                  (Hotspot: 1800, C:90, Churn:20)
   2. report/report_data_collector.py  (Hotspot: 312, C:26, Churn:12)
   3. kpis/codechurn/code_churn.py     (Hotspot: 297, C:27, Churn:11)

ğŸ’¡ ACTIONS
   - View detailed file tree:  --output-format human-tree
   - Full hotspot analysis:    --list-hotspots
   - Review strategy:          --review-strategy
   - Open HTML report:         open output/complexity_report.html

â±ï¸  Analysis: 1.04s | Reports: output/
```

**Benefits:**
- Quick overview by default
- Clear next steps for more detail
- Doesn't overwhelm with information
- Progressive disclosure of complexity

**Implementation:**
- Make this the default `--output-format human`
- Add flags to show more/less detail
- Interactive mode: prompt "Show details? [y/N]"

### Additional Format Ideas

#### Option 6: CI/CD Optimized Format

Designed for build pipelines:

```
METRICMANCER ANALYSIS - PASS âœ…

Quality Gate Results:
â”œâ”€ Critical Hotspots:  6/10 allowed  âœ… PASS
â”œâ”€ Average Complexity: 16.9/20 max   âœ… PASS
â”œâ”€ New Hotspots:       2 introduced  âš ï¸  WARNING
â””â”€ Code Coverage:      Unknown       âšª SKIP

Exit Code: 0 (use --strict for fail-on-warning)
```

**Use Case:** Perfect for CI/CD pipelines with quality gates

#### Option 7: Diff Mode (Compare Branches)

```
ğŸ“Š BRANCH COMPARISON: feature/new-analyzer vs main

Changes:
â”œâ”€ Files Modified:    12
â”œâ”€ Complexity Delta:  +15 (â†‘ 8.9%)
â”œâ”€ New Hotspots:      2
â””â”€ Resolved Hotspots: 1

âš ï¸  Complexity increased in:
   - analyzer.py      (+10, was C:80, now C:90)
   - scanner.py       (+5, was C:15, now C:20)

âœ… Improvements:
   - report_writer.py (-3, was C:8, now C:5)
```

**Use Case:** PR reviews, feature branch analysis

### Implementation Priority

**Phase 1 (High Priority) - v2.0:**
1. âœ… Executive Summary Dashboard as new default
2. âœ… `--summary` / `--detailed` flags
3. âœ… Keep current tree as `--output-format human-tree`
4. âœ… Top Risks Table (`--top-risks N`)

**Phase 2 (Medium Priority) - v2.1:**
5. â³ Quick Win Suggestions (`--quick-wins`)
6. â³ CI/CD optimized format (`--output-format ci`)
7. â³ Hybrid format with expandable details
8. â³ Color support with colorama (see below)

**Phase 3 (Future) - v3.0:**
9. ğŸ“‹ Trend Analysis (requires history storage)
10. ğŸ“‹ Diff Mode (branch comparison)
11. ğŸ“‹ Interactive terminal UI (with prompts)

### Configuration

Allow users to set their preferred default in `.metricmancer.yml`:

```yaml
output:
  terminal_format: summary  # summary, tree, risks, quick-wins, ci
  show_top_risks: 10
  colorize: true
  emoji: true
  show_recommendations: true
```

Or via CLI:

```bash
# Set default format
python -m src.main src --set-default-format summary

# Use specific format once
python -m src.main src --output-format summary
python -m src.main src --output-format risks --top 15
python -m src.main src --output-format tree  # classic view
```

### Backward Compatibility

- Current tree format always available via `--output-format human-tree` or `--output-format tree`
- Add `--legacy` flag for exact old behavior
- Document migration in CHANGELOG
- Environment variable: `METRICMANCER_OUTPUT_FORMAT=tree` for legacy users

### Color Support Enhancement

#### Current State
Terminal output currently uses emojis and Unicode box-drawing characters for visual hierarchy, but no ANSI color codes.

#### Proposed: Add Colorama Integration

Use **colorama** library for cross-platform color support:

```python
from colorama import Fore, Back, Style, init

# Initialize colorama for Windows compatibility
init(autoreset=True)

def print_colored_summary():
    """Enhanced summary with colors."""
    print(f"{Fore.CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"{Fore.CYAN}â•‘  {Style.BRIGHT}METRICMANCER ANALYSIS SUMMARY{Style.NORMAL}  â•‘")
    print(f"{Fore.CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Style.RESET_ALL}\n")
    
    # Critical issues in RED
    print(f"{Fore.RED}{Style.BRIGHT}ğŸ”¥ CRITICAL ISSUES{Style.RESET_ALL}")
    print(f"   {Fore.RED}Critical Hotspots:     7 files{Style.RESET_ALL}")
    print(f"   {Fore.RED}1. app/analyzer.py (Hotspot: 1800){Style.RESET_ALL}")
    
    # Warnings in YELLOW
    print(f"\n{Fore.YELLOW}{Style.BRIGHT}âš ï¸  HIGH PRIORITY{Style.RESET_ALL}")
    print(f"   {Fore.YELLOW}Emerging Hotspots:     3 files{Style.RESET_ALL}")
    
    # Success in GREEN
    print(f"\n{Fore.GREEN}{Style.BRIGHT}âœ… RECOMMENDATIONS{Style.RESET_ALL}")
    print(f"   {Fore.GREEN}1. Refactor analyzer.py{Style.RESET_ALL}")
```

**Color Scheme:**
- ğŸ”´ **Critical/Errors**: `Fore.RED` + `Style.BRIGHT`
- ğŸŸ¡ **Warnings/High Priority**: `Fore.YELLOW` + `Style.BRIGHT`
- ğŸŸ¢ **Success/Good**: `Fore.GREEN`
- ğŸ”µ **Info/Neutral**: `Fore.CYAN`
- âšª **Metrics/Data**: Default + `Style.DIM`
- ğŸ“Š **Headers**: `Style.BRIGHT`

**Benefits:**
- âœ… Improved visual hierarchy
- âœ… Faster scanning (color attracts attention)
- âœ… Cross-platform support (Windows, Linux, macOS)
- âœ… Graceful degradation (falls back to no color if unsupported)
- âœ… Accessibility: colors supplement (not replace) emojis and text

**Implementation:**

```python
class ColorSupport:
    """Handles color output with automatic detection and fallback."""
    
    def __init__(self):
        self.enabled = self._detect_color_support()
        if self.enabled:
            from colorama import Fore, Style, init
            init(autoreset=True)
            self.Fore = Fore
            self.Style = Style
        else:
            # No-op color codes
            self.Fore = type('Fore', (), {
                'RED': '', 'YELLOW': '', 'GREEN': '', 
                'CYAN': '', 'RESET': ''
            })()
            self.Style = type('Style', (), {
                'BRIGHT': '', 'DIM': '', 'RESET_ALL': '', 'NORMAL': ''
            })()
    
    def _detect_color_support(self) -> bool:
        """Detect if terminal supports color output."""
        import sys
        import os
        
        # Check environment variables
        if os.getenv('NO_COLOR'):
            return False
        if os.getenv('FORCE_COLOR'):
            return True
        
        # Check if output is to a TTY
        if not hasattr(sys.stdout, 'isatty'):
            return False
        if not sys.stdout.isatty():
            return False  # Piped output, no color
        
        # Check TERM variable
        term = os.getenv('TERM', '')
        if term in ['dumb', 'unknown']:
            return False
        
        return True  # Assume color support

# Usage in CLISummaryFormat
class CLISummaryFormat(ReportFormatStrategy):
    def __init__(self):
        self.color = ColorSupport()
    
    def _print_header(self):
        """Print colored header."""
        c = self.color
        print()
        print(f"{c.Fore.CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print(f"{c.Fore.CYAN}â•‘  {c.Style.BRIGHT}METRICMANCER ANALYSIS SUMMARY{c.Style.NORMAL}  â•‘")
        print(f"{c.Fore.CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{c.Style.RESET_ALL}")
        print()
```

**Configuration:**

Add to CLI arguments:
```bash
# Force enable colors
python -m src.main src --color

# Disable colors (for CI/CD or piping)
python -m src.main src --no-color

# Auto-detect (default)
python -m src.main src
```

Add to `.metricmancer.yml`:
```yaml
output:
  color: auto  # auto, always, never
  color_scheme: default  # default, high-contrast, accessibility
```

**Accessibility Considerations:**
1. **Never rely on color alone** - always use emojis + text
2. **High contrast mode** - option for color-blind users
3. **Respect NO_COLOR environment variable** (standard: https://no-color.org/)
4. **Provide color-free alternative** - `--no-color` flag
5. **Test with screen readers** - ensure color codes don't interfere

**Dependencies:**
```toml
# pyproject.toml
[project]
dependencies = [
    "colorama>=0.4.6",  # Cross-platform color support
]
```

**Testing:**
```python
def test_color_output_with_tty():
    """Test colored output when TTY is available."""
    with patch('sys.stdout.isatty', return_value=True):
        formatter = CLISummaryFormat()
        assert formatter.color.enabled is True

def test_color_output_without_tty():
    """Test plain output when piped."""
    with patch('sys.stdout.isatty', return_value=False):
        formatter = CLISummaryFormat()
        assert formatter.color.enabled is False

def test_no_color_env_variable():
    """Respect NO_COLOR environment variable."""
    with patch.dict('os.environ', {'NO_COLOR': '1'}):
        formatter = CLISummaryFormat()
        assert formatter.color.enabled is False
```

**Performance:**
- Colorama has minimal overhead (~1ms initialization)
- No performance impact on actual analysis
- Colors only applied during output formatting

**Example Output:**

Without colors (current):
```
ğŸ”¥ CRITICAL ISSUES (Immediate Attention Required)
   Critical Hotspots:     7 files
```

With colors (proposed):
```
ğŸ”¥ CRITICAL ISSUES (Immediate Attention Required)  [in bright RED]
   Critical Hotspots:     7 files                   [in RED]
```

**Fallback Behavior:**
- If colorama not installed: gracefully disable colors
- If terminal doesn't support colors: use plain text
- If output is piped: automatically disable colors
- If NO_COLOR env set: respect user preference

---

**Status**: â³ Planned for v2.1  
**Effort**: Low (1-2 days)  
**Value**: Medium (improves UX, not critical)  
**Dependencies**: colorama library  
**Backward Compatibility**: âœ… Fully compatible (colors are optional enhancement)

### User Research Needed

Before finalizing the default, consider:
1. **User surveys**: What do developers find most useful?
2. **Use case analysis**: Terminal vs CI/CD vs dashboard integration
3. **A/B testing**: Measure which format leads to more action
4. **Accessibility**: Ensure emoji/unicode can be disabled for screen readers
5. **Color preferences**: Survey users about color scheme preferences

---

**Status**: âœ… Implemented in v2.0 (Phase 1)  
**Effort**: Medium-High (1 week for Phase 1)  
**Value**: Very High (improves developer experience significantly)  
**Dependencies**: None (can implement incrementally)
